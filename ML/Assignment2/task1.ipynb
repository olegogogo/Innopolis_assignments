{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchmetrics import F1Score, Accuracy\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Assignment 2 in ML\n",
    "### Ostapovich Oleg"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Section 1: Data Reading"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('data/stream_quality_train.csv')\n",
    "data_test = pd.read_csv('data/stream_quality_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(760552, 12) (129978, 12)\n"
     ]
    }
   ],
   "source": [
    "print(data_train.shape, data_test.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Section 2: Exploration and preprocessing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "   fps_mean   fps_std  rtt_mean   rtt_std  dropped_frames_mean  \\\n0  0.744824  0.025512  0.786908  0.013918              0.00323   \n1  0.744824  0.025512  0.810122  0.055803              0.00323   \n2  0.734408  0.076537  0.791670  0.031781              0.00323   \n3  0.750031  0.000000  0.826193  0.015573              0.00323   \n4  0.703162  0.159856  0.816669  0.005438              0.00323   \n\n   dropped_frames_std  dropped_frames_max  bitrate_mean  bitrate_std  \\\n0                 0.0             0.00323      0.066147     0.010390   \n1                 0.0             0.00323      0.077022     0.041797   \n2                 0.0             0.00323      0.069172     0.017070   \n3                 0.0             0.00323      0.061703     0.021221   \n4                 0.0             0.00323      0.040957     0.062898   \n\n   packet_loss_rate  packet_loss_std    y  \n0          0.000250         0.000000  1.0  \n1          0.031492         0.153055  1.0  \n2          0.000250         0.000000  1.0  \n3          0.003121         0.014067  1.0  \n4          0.003121         0.014067  1.0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fps_mean</th>\n      <th>fps_std</th>\n      <th>rtt_mean</th>\n      <th>rtt_std</th>\n      <th>dropped_frames_mean</th>\n      <th>dropped_frames_std</th>\n      <th>dropped_frames_max</th>\n      <th>bitrate_mean</th>\n      <th>bitrate_std</th>\n      <th>packet_loss_rate</th>\n      <th>packet_loss_std</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.744824</td>\n      <td>0.025512</td>\n      <td>0.786908</td>\n      <td>0.013918</td>\n      <td>0.00323</td>\n      <td>0.0</td>\n      <td>0.00323</td>\n      <td>0.066147</td>\n      <td>0.010390</td>\n      <td>0.000250</td>\n      <td>0.000000</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.744824</td>\n      <td>0.025512</td>\n      <td>0.810122</td>\n      <td>0.055803</td>\n      <td>0.00323</td>\n      <td>0.0</td>\n      <td>0.00323</td>\n      <td>0.077022</td>\n      <td>0.041797</td>\n      <td>0.031492</td>\n      <td>0.153055</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.734408</td>\n      <td>0.076537</td>\n      <td>0.791670</td>\n      <td>0.031781</td>\n      <td>0.00323</td>\n      <td>0.0</td>\n      <td>0.00323</td>\n      <td>0.069172</td>\n      <td>0.017070</td>\n      <td>0.000250</td>\n      <td>0.000000</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.750031</td>\n      <td>0.000000</td>\n      <td>0.826193</td>\n      <td>0.015573</td>\n      <td>0.00323</td>\n      <td>0.0</td>\n      <td>0.00323</td>\n      <td>0.061703</td>\n      <td>0.021221</td>\n      <td>0.003121</td>\n      <td>0.014067</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.703162</td>\n      <td>0.159856</td>\n      <td>0.816669</td>\n      <td>0.005438</td>\n      <td>0.00323</td>\n      <td>0.0</td>\n      <td>0.00323</td>\n      <td>0.040957</td>\n      <td>0.062898</td>\n      <td>0.003121</td>\n      <td>0.014067</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "According Pandas Profiling, datasets are free from outliers, data is normalized, all categorical features are encoded. That's why the only thing left is separate Y column from other data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "y_train_df = data_train['y']\n",
    "x_train_df = data_train.drop(['y'], axis=1)\n",
    "\n",
    "y_test = data_test['y']\n",
    "x_test = data_test.drop(['y'], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Also for future tasks we need to find correlation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "fps_mean               0.182148\nfps_std                0.051231\nrtt_mean               0.237470\nrtt_std                0.022025\ndropped_frames_mean    0.011221\ndropped_frames_std     0.009314\ndropped_frames_max     0.005493\nbitrate_mean           0.630298\nbitrate_std            0.196570\npacket_loss_rate       0.037425\npacket_loss_std        0.052503\ny                      1.000000\nName: y, dtype: float64"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(data_train.corr().y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we can see that 'bitrate_mean', 'bitrate_std', 'rtt_mean', 'fps_mean' columns have the most correlation coefficient to Y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# removing all unnecessary features\n",
    "selected_features = ['bitrate_mean', 'bitrate_std', 'rtt_mean', 'fps_mean']\n",
    "x_train_df, y_train_df, x_test, y_test = x_train_df[selected_features], y_train_df, x_test[selected_features], y_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before split (760552, 4) (760552,) (129978, 4) (129978,)\n",
      "after split (608441, 4) (608441,) (152111, 4) (152111,) (129978, 4) (129978,)\n"
     ]
    }
   ],
   "source": [
    "# data splitting\n",
    "print(\"before split\", x_train_df.shape, y_train_df.shape, x_test.shape, y_test.shape)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_df, y_train_df, train_size=0.8)\n",
    "print(\"after split\", x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class Dset(Dataset):\n",
    "    def __init__(self, values, labels):\n",
    "        self.x = torch.tensor(values.values, dtype= torch.float32)\n",
    "        # self.y = torch.from_numpy(labels.values).type(torch.LongTensor)\n",
    "        self.y = list(map(int, labels.values))\n",
    "    def __len__(self):\n",
    "        return (len(self.y))\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i], self.y[i]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "# Adding data to loader\n",
    "trainloader = torch.utils.data.DataLoader(Dset(x_train,y_train), batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=0\n",
    "                                          )\n",
    "valloader = torch.utils.data.DataLoader(Dset(x_val,y_val), batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                        num_workers=0\n",
    "                                        )\n",
    "testloader = torch.utils.data.DataLoader(Dset(x_test,y_test), batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                         num_workers=0\n",
    "                                         )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def train_evaluate(net, optimizer, writer, epochs):\n",
    "    starttime = time.time()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # criterion = nn.NLLLoss()\n",
    "    accuracy_func = Accuracy(num_classes=3, average='weighted').to(device)\n",
    "    f1_score_func = F1Score(num_classes=3, average='weighted').to(device)\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        running_loss = 0.0\n",
    "        f1 = 0\n",
    "        accuracy = 0\n",
    "        for data in trainloader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, pred = torch.max(outputs.data, 1)\n",
    "            accuracy += accuracy_func(pred, labels) * 100\n",
    "            f1 += f1_score_func(pred, labels) * 100\n",
    "\n",
    "        running_loss /= len(trainloader)\n",
    "        accuracy /= len(trainloader)\n",
    "        f1 /= len(trainloader)\n",
    "        writer.add_scalar('Training_Loss', running_loss, epoch)\n",
    "        writer.add_scalar('Training_Accuracy', accuracy, epoch)\n",
    "        writer.add_scalar('Training_F1', f1, epoch)\n",
    "\n",
    "        print('Epoch {} - train loss:{}, accuracy:{}, f1_score:{}, time passed {}s'.format(epoch+1, running_loss, accuracy, f1, int(time.time()-starttime)))\n",
    "\n",
    "        val_loss = 0.0\n",
    "        val_accuracy = 0\n",
    "        val_f1_score = 0\n",
    "        with torch.no_grad():\n",
    "            for data in valloader:\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                _, pred = torch.max(outputs.data, 1)\n",
    "                val_loss += loss.item()\n",
    "                val_accuracy += accuracy_func(pred, labels) * 100\n",
    "                val_f1_score += f1_score_func(pred, labels) * 100\n",
    "            val_loss /= len(valloader)\n",
    "            val_accuracy /= len(valloader)\n",
    "            val_f1_score /= len(valloader)\n",
    "            writer.add_scalar('Val_Loss', val_loss, epoch)\n",
    "            writer.add_scalar('Val_Accuracy', val_accuracy, epoch)\n",
    "            writer.add_scalar('Val_F1', val_f1_score, epoch)\n",
    "\n",
    "        print('Epoch {} - val loss:{}, accuracy:{}, f1_score:{}, time passed {}s'.format(epoch+1, val_loss, val_accuracy, val_f1_score, int(time.time()-starttime)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "def test_evaluate(net, optimizer):\n",
    "    val_loss = 0.0\n",
    "    val_accuracy = 0\n",
    "    val_f1_score = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    accuracy_func = Accuracy(num_classes=3, average='weighted').to(device)\n",
    "    f1_score_func = F1Score(num_classes=3, average='weighted').to(device)\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, pred = torch.max(outputs.data, 1)\n",
    "            val_loss += loss.item()\n",
    "            val_accuracy += accuracy_func(pred, labels) * 100\n",
    "            val_f1_score += f1_score_func(pred, labels) * 100\n",
    "        val_loss /= len(testloader)\n",
    "        val_accuracy /= len(testloader)\n",
    "        val_f1_score /= len(testloader)\n",
    "    print('Test evaluation - test loss:{}, accuracy:{}, f1_score:{}'.format(val_loss, val_accuracy, val_f1_score))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Section 3.1: Machine learning or Deep learning model defining, training and hyper-parameters turning"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this section two base models and one ensemble model were created"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "device = torch.device(\"cuda:0\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "class StartModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StartModel, self).__init__()\n",
    "        self.network=nn.Sequential(\n",
    "            nn.Linear(in_features=4, out_features=3, bias=True))\n",
    "            # nn.LogSoftmax(dim=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "startmodel = StartModel().to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "class ImprovedModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImprovedModel, self).__init__()\n",
    "        self.network=nn.Sequential(\n",
    "            nn.Linear(in_features=4, out_features=200, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.Linear(in_features=200, out_features=100, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.Linear(in_features=100, out_features=50, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.Linear(in_features=50, out_features=3, bias=True),\n",
    "            nn.LogSoftmax(dim=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "improvedmodel = ImprovedModel().to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "class Ensembling(nn.Module):\n",
    "    def __init__(self, modelA, modelB):\n",
    "        super(Ensembling, self).__init__()\n",
    "        self.modelA = modelA\n",
    "        self.modelB = modelB\n",
    "        self.classifier = nn.Linear(6, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.modelA(x)\n",
    "        x2 = self.modelB(x)\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = self.classifier(F.relu(x))\n",
    "        return x\n",
    "ensembling = Ensembling(startmodel, improvedmodel).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Section 4.1: Model performance evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - train loss:0.5271992166363615, accuracy:81.0798568725586, f1_score:80.17546081542969, time passed 79s\n",
      "Epoch 1 - val loss:0.44192712858769645, accuracy:84.21276092529297, f1_score:84.09778594970703, time passed 93s\n",
      "Epoch 2 - train loss:0.4349664609968026, accuracy:84.39073181152344, f1_score:84.29925537109375, time passed 164s\n",
      "Epoch 2 - val loss:0.4330424742379107, accuracy:84.37444305419922, f1_score:84.27202606201172, time passed 186s\n",
      "Epoch 3 - train loss:0.4308922117253063, accuracy:84.55455780029297, f1_score:84.47653198242188, time passed 248s\n",
      "Epoch 3 - val loss:0.43159701849447757, accuracy:84.45854187011719, f1_score:84.34502410888672, time passed 262s\n",
      "Epoch 4 - train loss:0.4299958995648261, accuracy:84.61475372314453, f1_score:84.53621673583984, time passed 322s\n",
      "Epoch 4 - val loss:0.43106506816778484, accuracy:84.52024841308594, f1_score:84.45096588134766, time passed 335s\n",
      "Epoch 5 - train loss:0.42980250601114167, accuracy:84.61400604248047, f1_score:84.54107666015625, time passed 395s\n",
      "Epoch 5 - val loss:0.43085918827141473, accuracy:84.55303955078125, f1_score:84.47515106201172, time passed 409s\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adadelta(startmodel.parameters())\n",
    "writer = SummaryWriter('runs1/BaseModelbase2')\n",
    "train_evaluate(startmodel, optimizer, writer, 5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test evaluation - test loss:0.5310734307307463, accuracy:78.1305923461914, f1_score:76.17174530029297\n"
     ]
    }
   ],
   "source": [
    "# testing first model\n",
    "test_evaluate(startmodel, optimizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - train loss:0.4250807216080271, accuracy:84.07757568359375, f1_score:84.06288146972656, time passed 70s\n",
      "Epoch 1 - val loss:0.4297611152804634, accuracy:83.77128601074219, f1_score:83.61913299560547, time passed 83s\n",
      "Epoch 2 - train loss:0.42404856910789807, accuracy:84.2015609741211, f1_score:84.18572998046875, time passed 150s\n",
      "Epoch 2 - val loss:0.42351916195450257, accuracy:84.26537322998047, f1_score:84.23945617675781, time passed 162s\n",
      "Epoch 3 - train loss:0.42249655151034143, accuracy:84.23336791992188, f1_score:84.21257781982422, time passed 229s\n",
      "Epoch 3 - val loss:0.42559631884528837, accuracy:84.10763549804688, f1_score:84.15430450439453, time passed 242s\n",
      "Epoch 4 - train loss:0.4222891642578136, accuracy:84.2564697265625, f1_score:84.2392807006836, time passed 309s\n",
      "Epoch 4 - val loss:0.42187589666415765, accuracy:84.21466827392578, f1_score:84.18081665039062, time passed 321s\n",
      "Epoch 5 - train loss:0.422060643994818, accuracy:84.24461364746094, f1_score:84.22643280029297, time passed 387s\n",
      "Epoch 5 - val loss:0.42312768971089776, accuracy:84.29625701904297, f1_score:84.28968811035156, time passed 399s\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adadelta(improvedmodel.parameters())\n",
    "writer = SummaryWriter('runs1/ImprovedModel')\n",
    "train_evaluate(improvedmodel, optimizer, writer, 5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test evaluation - test loss:0.42100789078153095, accuracy:84.31564331054688, f1_score:84.35420989990234\n"
     ]
    }
   ],
   "source": [
    "# testing second model\n",
    "test_evaluate(improvedmodel, optimizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - train loss:0.4117550442452889, accuracy:84.95167541503906, f1_score:84.87709045410156, time passed 73s\n",
      "Epoch 1 - val loss:0.41250681546216883, accuracy:85.0295639038086, f1_score:84.96405792236328, time passed 86s\n",
      "Epoch 2 - train loss:0.4116206688673259, accuracy:84.96923828125, f1_score:84.89723205566406, time passed 167s\n",
      "Epoch 2 - val loss:0.41269609624821, accuracy:85.01170349121094, f1_score:84.95594787597656, time passed 185s\n",
      "Epoch 3 - train loss:0.41153104872172647, accuracy:84.96698760986328, f1_score:84.89115142822266, time passed 256s\n",
      "Epoch 3 - val loss:0.41389455632569755, accuracy:84.98285675048828, f1_score:84.95818328857422, time passed 269s\n",
      "Epoch 4 - train loss:0.41142231127524237, accuracy:84.98345947265625, f1_score:84.90711212158203, time passed 340s\n",
      "Epoch 4 - val loss:0.41243641238057505, accuracy:84.91064453125, f1_score:84.79624938964844, time passed 353s\n",
      "Epoch 5 - train loss:0.41136383243094926, accuracy:84.9690170288086, f1_score:84.89311981201172, time passed 436s\n",
      "Epoch 5 - val loss:0.4120585397962828, accuracy:85.01570129394531, f1_score:84.955322265625, time passed 456s\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adadelta(ensembling.parameters())\n",
    "writer = SummaryWriter('runs1/Ensembling1')\n",
    "train_evaluate(ensembling, optimizer, writer, 5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test evaluation - test loss:0.406222931341483, accuracy:84.96727752685547, f1_score:84.85663604736328\n"
     ]
    }
   ],
   "source": [
    "# testing ensembling\n",
    "test_evaluate(ensembling, optimizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Section 3.2: Machine learning or Deep learning model defining, training and hyper-parameters turning"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "randomforestfunc = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=9)\n",
    "randomforest = randomforestfunc.fit(x_train,y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Section 4.2: Model performance evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8517749157549739\n",
      "F1-score: 0.8513006286481531\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "y_pred = randomforest.predict(x_test) # model testing\n",
    "print(\"Accuracy:\", accuracy_score(y_test,y_pred))\n",
    "print(\"F1-score:\", f1_score(y_test,y_pred,average='weighted'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "fig = plt.figure(figsize=(20, 15))\n",
    "plot_tree(randomforest.estimators_[0], # creating image of first tree\n",
    "          feature_names=x_train[selected_features].columns,\n",
    "          filled=True, impurity=True,\n",
    "          rounded=True)\n",
    "writer = SummaryWriter('runs1/RandomForest') # write tree image to tensorboard\n",
    "writer.add_figure('First tree',fig)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Section 5: Conclusion and possible improvements"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 1.1\n",
    "The goal is to implement deep learning model and improve it's performance with ensembling learning.\n",
    "\n",
    "To do this, two models was created. First one is very simple with one FC layer. Second is much more complicated. This two models was connected together in ensemble model.\n",
    "\n",
    "As a result, accuracy and f1_score was close to 85%. Multiple approaches was used to achieve 90%. These attempts consist of creating different DNN structures with combination of FC, ReLU, BatchNormalization and Dropout layers as well as using different optimization and loss functions. Tuning of hyperparameters, such as learning rate, batch size and number of epochs also have no effect on results. Some of the best results are presented in tensorboard.\n",
    "\n",
    "According to tests, all used methods have no effect on metrics so may be it is possible to say that the highest model efficiency on this data was achieved.\n",
    "\n",
    "### Task 2.2\n",
    "The goal is to select, train and evaluate an appropriate ML model that will provide an ability to understand why a specific prediction was achieved.\n",
    "\n",
    "Decision tree was used to provide this goal. To improve model performance was used 100 of trees in ensemble. RandomForest parameters criterion='gini' and max_depth=9 was chosen due to a variety of tests, according to the results of which, the model got 85% accuracy and f1_score.\n",
    "\n",
    "Example of decision tree from RandomForest presented in tensorboard.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": "Launching TensorBoard..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n      <iframe id=\"tensorboard-frame-e5ca43fa97a1215d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n      </iframe>\n      <script>\n        (function() {\n          const frame = document.getElementById(\"tensorboard-frame-e5ca43fa97a1215d\");\n          const url = new URL(\"/\", window.location);\n          const port = 6006;\n          if (port) {\n            url.port = port;\n          }\n          frame.src = url;\n        })();\n      </script>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}