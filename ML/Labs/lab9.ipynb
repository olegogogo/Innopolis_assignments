{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lLYOztuGtZQr"
   },
   "source": [
    "# Lab 9: Techniques for training Deep Neural Netwoks\n",
    "\n",
    "```\n",
    "- Machine Learning, Innopolis University (Fall semester 2022)\n",
    "- Professor: Adil Khan\n",
    "- Teaching Assistant: Gcinizwe Dlamini\n",
    "```\n",
    "<hr>\n",
    "\n",
    "\n",
    "```\n",
    "In this lab, you will practice techniques that are used to improve deep learning models perfomence in Pytorch.\n",
    "\n",
    "Lab Plan\n",
    "1. Data Augmentation examples\n",
    "2. Batch normalization, Dropout, ...\n",
    "3. Adaptive Learning rate and Optimizers\n",
    "4. Using TensorBoard\n",
    "5. Using Pretrained models (Transfer learning)\n",
    "\n",
    "```\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcGwL_d__qH6"
   },
   "source": [
    "# 1. CNN with PyTorch\n",
    "\n",
    "## Load data and preprocess\n",
    "\n",
    "To load our data set (CIFAR10) we can either download it manualy or use torchvision package. `torchvision` consists of popular datasets, model architectures, and common image transformations for computer vision.\n",
    "\n",
    "\n",
    "For smooth training of the CNN we need to transform (normalize) the images. `transforms` package conatains common image transformations. Image transformations can be chained together using `transforms.Compose`. In our case we need to first convert image to tensor `transforms.ToTensor()` then normalize `transforms.Normalize`\n",
    "\n",
    "The packages can be pip installed<br>\n",
    "`!pip install torch` <br>\n",
    "`!pip install torchvision`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138,
     "referenced_widgets": [
      "7d2acab730e340328275a65d7aad46cb",
      "648f2e5785c94231a90fcdae2b6c4259",
      "61da3848bcbc4ac4b390a0dd933cfc2e",
      "b3fc7d5e4ba743878e72117ccbb59921",
      "dba3174329364f9c997be3ed52805898",
      "234a4d772c654ade8fa221d5a82fe986",
      "e2873a5ff41a44038f163e8345203ebf",
      "51310412110a41d89b328e96228c0fe5",
      "d609c3e38828488cbe102d54b652f25b",
      "46ac91c05ac04b4286239ea3374463f8",
      "085cbcb85d084380a11e730436658be3"
     ]
    },
    "id": "X_PBgoYvBzzw",
    "outputId": "4770839c-aa59-46b2-f230-bf536f912d58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "batch_size = 128\n",
    "test_batch_size = 100\n",
    "\n",
    "# Transformations\n",
    "# TODO : add data augmentation of your choice\n",
    "transform_train = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.RandomVerticalFlip(0.5),\n",
    "     transforms.RandomRotation(0.3),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "transform_test = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# Create Train Dataloader\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "trainset1 = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True)\n",
    "trainloader1 = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "#Create Test Dataloader\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=test_batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcV94hYjkXg0"
   },
   "source": [
    "## Data visualization \n",
    "Create a simple method to have a look at the data (image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 155
    },
    "id": "kKCNlbJYkXg1",
    "outputId": "0b7ef5dc-2f17-4bc0-9a4c-2e93e4667794"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_MultiProcessingDataLoaderIter' object has no attribute 'next'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [2], line 12\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# get some random training images\u001B[39;00m\n\u001B[0;32m     11\u001B[0m dataiter \u001B[38;5;241m=\u001B[39m \u001B[38;5;28miter\u001B[39m(trainloader)\n\u001B[1;32m---> 12\u001B[0m images, labels \u001B[38;5;241m=\u001B[39m \u001B[43mdataiter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnext\u001B[49m()\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m# show images\u001B[39;00m\n\u001B[0;32m     15\u001B[0m imshow(torchvision\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mmake_grid(images[:\u001B[38;5;241m4\u001B[39m]))\n",
      "\u001B[1;31mAttributeError\u001B[0m: '_MultiProcessingDataLoaderIter' object has no attribute 'next'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images[:4]))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kaTWC0_kXg1"
   },
   "source": [
    "## Lets Switch to the GPU (if available) - why?\n",
    "\n",
    "\n",
    "Unlike TensorFlow, PyTorch doesn’t have a dedicated library for GPU users,so its neccessary to some manual setup <br>\n",
    "<b>NB: </b>If working on Colab, make sure that GPU runtime is enabled <br>\n",
    "<b>NB: </b>It’s not possible to transfer Data Loaders directly to GPU <br>\n",
    "<b>NB: </b> To get total number of GPU devices available `torch.cuda.device_count()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uj3zcLftkXg2",
    "outputId": "be087667-c677-48a9-cb80-4470689d1809"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assume that we are on a CUDA machine, then this should print a CUDA device:\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#TODO: Create the BaseModel by defining the missing (None) convolutional layers, Max pool and fully connected layers\n",
    "#TODO: Finish implementation of foward method in class BaseModel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1) \n",
    "        \n",
    "        # TODO \n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # TODO\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "\n",
    "        # TODO\n",
    "        self.fc1 = nn.Linear(8*8*128, 256)\n",
    "        self.fc2 = nn.Linear(256, 84)\n",
    "        self.output = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Implement the foward pass method. NOTE: Apply Relu activation function to all layers\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(F.relu(self.conv2(x)))\n",
    "        x = self.pool2(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 8*8*128)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.output(x)\n",
    "        # order : conv1 -> activation -> conv2 -> activation -> pool1 -> conv3 -> activation -> pool2 -> fc1 -> activation -> fc2 -> activation -> output\n",
    "\n",
    "model = BaseModel().to(device)"
   ],
   "metadata": {
    "id": "hd0FnImXkXg3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZpsM2vS4F26n"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "R1sHQhqFF4Pc",
    "outputId": "94edae09-67f4-4e62-deb3-a386332b4297"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of params in the CNN model: 2213094\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of params in the CNN model:\", count_parameters(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pry2xqvdDDGO"
   },
   "source": [
    "## 3. Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yS3Eo37FDKt3"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "def train_evaluate(net, optimizer, writer):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0): #TODO: Iterate through the batches and train the cnn\n",
    "            # TODO : get the inputs and place them to the training devide (i.e GPU or CPU)\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # TODO : Train procedure -> zero the gradients, forward pass + backward pass + optimize\n",
    "            optimizer.zero_grad()\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # TODO: print statistics and log loss to Tensorboard\n",
    "        running_loss /= len(trainloader)\n",
    "        optimizer = torch.optim.Adam(model.parameters())\n",
    "        writer.add_scalar('Loss/train', running_loss, epoch)\n",
    "        writer.add_scalar('Loss/test', running_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/train', running_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/test', running_loss, epoch)\n",
    "\n",
    "    \n",
    "    #Print Model overall Accuracy \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n",
    "    \n",
    "    #Print Model classwise Accuracy \n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            c = (predicted == labels).squeeze()\n",
    "            for i in range(4):\n",
    "                label = labels[i]\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "\n",
    "\n",
    "    for i in range(10):\n",
    "        print('Accuracy of %5s : %2d %%' % (\n",
    "            classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O15u3Rm1kXg6"
   },
   "source": [
    "## Simple CNN training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "V_CvS8oBDhL1",
    "outputId": "7c86beb1-139a-4595-a216-d4c8a3896139"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [37], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# TODO : define an optimizer SGD with learning rate 0.001 and momentum 0.9\u001B[39;00m\n\u001B[0;32m      2\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mAdam(model\u001B[38;5;241m.\u001B[39mparameters())\n\u001B[1;32m----> 3\u001B[0m \u001B[43mtrain_evaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn [36], line 16\u001B[0m, in \u001B[0;36mtrain_evaluate\u001B[1;34m(net, optimizer)\u001B[0m\n\u001B[0;32m     14\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m# forward + backward + optimize\u001B[39;00m\n\u001B[1;32m---> 16\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mnet\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     17\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, labels)\n\u001B[0;32m     19\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[1;32mD:\\Innopolis\\_my\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[1;32mIn [25], line 26\u001B[0m, in \u001B[0;36mBaseModel.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m     25\u001B[0m     \u001B[38;5;66;03m# TODO: Implement the foward pass method. NOTE: Apply Relu activation function to all layers \u001B[39;00m\n\u001B[1;32m---> 26\u001B[0m     x \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     27\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpool1(F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv2(x)))\n\u001B[0;32m     28\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpool2(F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv3(x)))\n",
      "File \u001B[1;32mD:\\Innopolis\\_my\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mD:\\Innopolis\\_my\\env\\lib\\site-packages\\torch\\nn\\modules\\conv.py:457\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    456\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 457\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Innopolis\\_my\\env\\lib\\site-packages\\torch\\nn\\modules\\conv.py:453\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[1;34m(self, input, weight, bias)\u001B[0m\n\u001B[0;32m    449\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    450\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[0;32m    451\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[0;32m    452\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[1;32m--> 453\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    454\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# TODO : define an optimizer SGD with learning rate 0.001 and momentum 0.9\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "train_evaluate(model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EvWOkO-LkXg6"
   },
   "source": [
    "## Improved CNN Model\n",
    "\n",
    "### Task \n",
    "1. Add Dropout for both convolution and fully connected layers\n",
    "2. Add Batch Normalization for convolutions\n",
    "3. Add Batch Normalization for fully connected network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GBvgKwS9kXg7"
   },
   "outputs": [],
   "source": [
    "#TODO: Create the ImprovedModel by defining the missing (None) batch Normalization layers and Dropout\n",
    "class ImprovedModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImprovedModel, self).__init__()\n",
    "        self.network=nn.Sequential(\n",
    "            nn.Conv2d(3,32,3,padding=1),\n",
    "            None , # Batch Normalization\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            None,  # Dropout 25%\n",
    "            \n",
    "            nn.Conv2d(32,64,3,padding=1),\n",
    "            None, # Batch Normalization\n",
    "            nn.ReLU(),\n",
    "            None,  # Dropout 25%\n",
    "            \n",
    "            nn.Conv2d(64,128,3,padding=1),\n",
    "            None, # Batch Normalization\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            None,  # Dropout 25%\n",
    "            \n",
    "            nn.Conv2d(128,128,3,padding=1),\n",
    "            None , # Batch Normalization\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            nn.Conv2d(128,128,3,padding=1),\n",
    "            None, # Batch Normalization\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            None, # Dropout 25%\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(None),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,10))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "model_improved = ImprovedModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JuLn_ugkkXg7",
    "outputId": "d7a49356-86c1-4bfb-e8d1-7f316bcb9cb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of params in the CNN model: 2213094\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of params in the CNN model:\", count_parameters(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3s8gZaoBkXg7"
   },
   "source": [
    "## Improved CNN training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GPzu6YJWkXg8",
    "outputId": "cb6b45e4-782e-421f-8944-2dad34012230"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Loss 1.5229\n",
      "[Epoch 2] Loss 1.2268\n",
      "[Epoch 3] Loss 1.0909\n",
      "[Epoch 4] Loss 1.0057\n",
      "[Epoch 5] Loss 0.9471\n",
      "[Epoch 6] Loss 0.8981\n",
      "[Epoch 7] Loss 0.8546\n",
      "[Epoch 8] Loss 0.8245\n",
      "[Epoch 9] Loss 0.7930\n",
      "[Epoch 10] Loss 0.7744\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 test images: 72 %\n",
      "Accuracy of plane : 84 %\n",
      "Accuracy of   car : 87 %\n",
      "Accuracy of  bird : 52 %\n",
      "Accuracy of   cat : 72 %\n",
      "Accuracy of  deer : 61 %\n",
      "Accuracy of   dog : 68 %\n",
      "Accuracy of  frog : 88 %\n",
      "Accuracy of horse : 84 %\n",
      "Accuracy of  ship : 64 %\n",
      "Accuracy of truck : 79 %\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model_improved.parameters(),lr=0.001)\n",
    "train_evaluate(model_improved, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1iIuAHZkXg8"
   },
   "source": [
    "## CNN from popular model architectures (Transfer learning)\n",
    "\n",
    "Same procedure as simple CNN but the achitecture of the neural net is much more complicated and the weights are pretrained. <br> \n",
    "\n",
    "\n",
    "### Fine Tuning\n",
    "Fine tuning or transfer learning is the task of training a model on a big dataset and then adjusting the parameters of the model for a smaller task with less data.\n",
    "\n",
    "It is very common in Computer Vision and Natural Language Processing with the immergence of BERT and UMLfit. <br>\n",
    "![](https://miro.medium.com/max/1276/1*ZkPBqU8vx2vAgcLpz9pi5g.jpeg)\n",
    "\n",
    "Say we want to use resnet (trained on imagenet with 1000 classes and has input shape of 256x256)  and fine-tune it for CIFAR10 (has 10 categories with input shape of 32×32)\n",
    "\n",
    "What is the biggest changes that we need to make?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGbfg0BBkXg8"
   },
   "source": [
    "## How to Import Popular CNN models? \n",
    "\n",
    "`torchvision,models` contains of popular model architectures and can be loaded together with their trained weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GTs9YicZkXg8"
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "# resnet18 = models.resnet18(pretrained=True,progress=True)\n",
    "# alexnet = models.alexnet(pretrained=True,progress=True)\n",
    "# squeezenet = models.squeezenet1_0(pretrained=True,progress=True)\n",
    "# vgg16 = models.vgg16(pretrained=True,progress=True)\n",
    "# densenet = models.densenet161(pretrained=True,progress=True)\n",
    "# inception = models.inception_v3(pretrained=True,progress=True)\n",
    "# googlenet = models.googlenet(pretrained=True,progress=True)\n",
    "# shufflenet = models.shufflenet_v2_x1_0(pretrained=True,progress=True)\n",
    "# mobilenet = models.mobilenet_v2(pretrained=True,progress=True)\n",
    "# resnext50_32x4d = models.resnext50_32x4d(pretrained=True,progress=True)\n",
    "# wide_resnet50_2 = models.wide_resnet50_2(pretrained=True,progress=True)\n",
    "# mnasnet = models.mnasnet1_0(pretrained=True,progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rk7s5TF7kXg9"
   },
   "source": [
    "## Start by loading and transforming the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fo9ePoPDkXg9",
    "outputId": "804018bd-832b-4c87-efa1-ffd3f6791af0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Resize((256,256)),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4UQ5vf6kXg9"
   },
   "source": [
    "## Define CNN model \n",
    "\n",
    "<b>NB : </b>To make the model architecture fit our classification task, we will change the last fully connected layer and train only it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1-ReM_G4kXg9"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = models.resnet18(weights='IMAGENET1K_V1', progress=True)\n",
    "        self.net.trainable = False # Freeze the layers of the pretrained model\n",
    "        self.net.fc = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "net = Net()\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4grX4vP3kXg-"
   },
   "source": [
    "## Self-practice Task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKUyMnXHkXg-"
   },
   "source": [
    "1. Add Tensorboard to monitor the train and validation loss, recall, precision and accuracy every after training epoch\n",
    "2. Create another CNN using pretrained model and compare it with already implemented resnet and simple CNN (plot their losses on same tensorboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task1"
   ],
   "metadata": {
    "collapsed": false,
    "id": "ZSFVaIxrkXg-"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchmetrics import Recall, Precision, Accuracy \n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import  time\n",
    "\n",
    "batch_size = 128\n",
    "test_batch_size = 128\n",
    "\n",
    "# Transformations\n",
    "# TODO : add data augmentation of your choice\n",
    "transform_train = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.RandomVerticalFlip(0.5),\n",
    "     transforms.RandomRotation(0.3),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "transform_test = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# Create Train Dataloader\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "#Create Test Dataloader\n",
    "valset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform_test)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=test_batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ],
   "metadata": {
    "id": "BtZlT7clkXg-",
    "outputId": "11f1c088-4c55-4c8d-faab-5cf02ffc5a25",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101,
     "referenced_widgets": [
      "9f92ed155f904c75a3769753b7dc2af3",
      "09630dae76034a4a92a58bc35c21de40",
      "549f9fd7e8054f53bba76835de79f058",
      "2e23b10ab2e548388c3ff12b82183980",
      "a04747107b7548b3a40229b7f6c57666",
      "03cba228644547e6b4a737fc1cbf9ec1",
      "9ae111eaa11f42f8a3fce2954f175adb",
      "ea935600ebca40a2be8b33734ddff5f8",
      "88c9652c16d547b09e33c515d790c952",
      "48e37ef977304ea9a41e325819e34be4",
      "4f3fdbcbc3fa4542abf12925a0611ece"
     ]
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assume that we are on a CUDA machine, then this should print a CUDA device:\n",
    "\n",
    "print(device)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GKqWTPT8orjO",
    "outputId": "9b22fbac-9eee-44f0-a87d-6f9151e5d44e"
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 32, 32]             896\n",
      "            Conv2d-2           [-1, 64, 32, 32]          18,496\n",
      "         MaxPool2d-3           [-1, 64, 16, 16]               0\n",
      "            Conv2d-4          [-1, 128, 16, 16]          73,856\n",
      "         MaxPool2d-5            [-1, 128, 8, 8]               0\n",
      "            Linear-6                  [-1, 256]       2,097,408\n",
      "            Linear-7                   [-1, 84]          21,588\n",
      "            Linear-8                   [-1, 10]             850\n",
      "================================================================\n",
      "Total params: 2,213,094\n",
      "Trainable params: 2,213,094\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.19\n",
      "Params size (MB): 8.44\n",
      "Estimated Total Size (MB): 9.64\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "\n",
    "        # TODO\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # TODO\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # TODO\n",
    "        self.fc1 = nn.Linear(8 * 8 * 128, 256)\n",
    "        self.fc2 = nn.Linear(256, 84)\n",
    "        self.output = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Implement the foward pass method. NOTE: Apply Relu activation function to all layers\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(F.relu(self.conv2(x)))\n",
    "        x = self.pool2(F.relu(self.conv3(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.output(x)\n",
    "        # order : conv1 -> activation -> conv2 -> activation -> pool1 -> conv3 -> activation -> pool2 -> fc1 -> activation -> fc2 -> activation -> output\n",
    "\n",
    "model = BaseModel().to(device)\n",
    "summary(model, (3, 32, 32))"
   ],
   "metadata": {
    "id": "Fxo6PIM_kXg_"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "def train_evaluate(net, optimizer, writer):\n",
    "    starttime = time.time()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    accuracy_func = Accuracy(num_classes=10, average='weighted').to(device)\n",
    "    recall_func = Recall().to(device)\n",
    "    precision_func = Precision().to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        running_loss = 0.0\n",
    "        accuracy = 0\n",
    "        recall = 0\n",
    "        precision = 0\n",
    "        for data in trainloader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, pred = torch.max(outputs.data, 1)\n",
    "            accuracy += accuracy_func(pred, labels) * 100\n",
    "            recall += recall_func(pred, labels) * 100\n",
    "            precision += precision_func(pred, labels) * 100\n",
    "\n",
    "        running_loss /= len(trainloader)\n",
    "        accuracy /= len(trainloader)\n",
    "        recall /= len(trainloader)\n",
    "        precision /= len(trainloader)\n",
    "        writer.add_scalar('Training_Loss', running_loss, epoch)\n",
    "        writer.add_scalar('Training_Accuracy', accuracy, epoch)\n",
    "        writer.add_scalar('Training_Recall', recall, epoch)\n",
    "        writer.add_scalar('Training_Precision', precision, epoch)\n",
    "\n",
    "\n",
    "        print('Epoch {} - train loss:{}, accuracy:{}, time passed {}s'.format(epoch+1, running_loss, accuracy, int(time.time()-starttime)))\n",
    "\n",
    "        val_loss = 0.0\n",
    "        val_accuracy = 0\n",
    "        val_recall = 0\n",
    "        val_precision = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in valloader:\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                _, pred = torch.max(outputs.data, 1)\n",
    "                val_loss += loss.item()\n",
    "                val_accuracy += accuracy_func(pred, labels) * 100\n",
    "                val_recall += recall_func(pred, labels) * 100\n",
    "                val_precision += precision_func(pred, labels) * 100\n",
    "\n",
    "            val_loss /= len(valloader)\n",
    "            val_accuracy /= len(valloader)\n",
    "            val_recall /= len(valloader)\n",
    "            val_precision /= len(valloader)\n",
    "\n",
    "            writer.add_scalar('Val_Loss', val_loss, epoch)\n",
    "            writer.add_scalar('Val_Accuracy', val_accuracy, epoch)\n",
    "            writer.add_scalar('Val_Recall', val_recall, epoch)\n",
    "            writer.add_scalar('Val_Precision', val_precision, epoch)\n",
    "\n",
    "        print('Epoch {} - val loss:{}, accuracy:{}, time passed {}s'.format(epoch+1, val_loss, val_accuracy,  int(time.time()-starttime)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - train loss:1.0569633068635946, accuracy:62.06441879272461, time passed 45s\n",
      "Epoch 1 - val loss:0.9887787639340267, accuracy:64.6756362915039, time passed 54s\n",
      "Epoch 2 - train loss:0.9053687895350444, accuracy:67.8960189819336, time passed 101s\n",
      "Epoch 2 - val loss:0.9045860676825801, accuracy:67.78085327148438, time passed 112s\n",
      "Epoch 3 - train loss:0.782787695260304, accuracy:72.30298614501953, time passed 168s\n",
      "Epoch 3 - val loss:0.8613802835911135, accuracy:69.47191619873047, time passed 176s\n",
      "Epoch 4 - train loss:0.6871593373510844, accuracy:75.64737701416016, time passed 225s\n",
      "Epoch 4 - val loss:0.8337543689751927, accuracy:70.72785186767578, time passed 236s\n",
      "Epoch 5 - train loss:0.6006651279871421, accuracy:78.84510803222656, time passed 283s\n",
      "Epoch 5 - val loss:0.8242959553682352, accuracy:71.46954345703125, time passed 291s\n",
      "Epoch 6 - train loss:0.5224344988003411, accuracy:81.63603210449219, time passed 335s\n",
      "Epoch 6 - val loss:0.839801056475579, accuracy:71.855224609375, time passed 344s\n",
      "Epoch 7 - train loss:0.44940597920314124, accuracy:84.39098358154297, time passed 388s\n",
      "Epoch 7 - val loss:0.8489767893960204, accuracy:73.4375, time passed 399s\n",
      "Epoch 8 - train loss:0.39579602687255194, accuracy:86.28157043457031, time passed 451s\n",
      "Epoch 8 - val loss:0.8617303062088882, accuracy:72.82437133789062, time passed 459s\n",
      "Epoch 9 - train loss:0.332879174472121, accuracy:88.48664855957031, time passed 511s\n",
      "Epoch 9 - val loss:0.8918763089783585, accuracy:72.4980239868164, time passed 520s\n",
      "Epoch 10 - train loss:0.28109344338898157, accuracy:90.4407730102539, time passed 566s\n",
      "Epoch 10 - val loss:0.969746894474271, accuracy:72.75514221191406, time passed 575s\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "writer = SummaryWriter('runs/lab9/simpleCNN')\n",
    "train_evaluate(model, optimizer, writer)"
   ],
   "metadata": {
    "id": "PKewtpKjkXhA",
    "outputId": "b4827d5d-2386-4af9-f803-6db148b5eb7d",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
      "             ReLU-37            [-1, 256, 2, 2]               0\n",
      "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
      "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
      "             ReLU-42            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
      "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
      "             ReLU-46            [-1, 256, 2, 2]               0\n",
      "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
      "             ReLU-49            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
      "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-53            [-1, 512, 1, 1]               0\n",
      "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-58            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-65            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                   [-1, 10]           5,130\n",
      "           ResNet-69                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 11,181,642\n",
      "Trainable params: 11,181,642\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.29\n",
      "Params size (MB): 42.65\n",
      "Estimated Total Size (MB): 43.95\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = models.resnet18(weights='IMAGENET1K_V1', progress=True)\n",
    "        self.net.trainable = False # Freeze the layers of the pretrained model\n",
    "        self.net.fc = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "net = Net()\n",
    "net = net.to(device)\n",
    "summary(net, (3, 32, 32))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - train loss:1.0669170686655947, accuracy:62.98553466796875, time passed 154s\n",
      "Epoch 1 - val loss:0.9242201704013197, accuracy:68.45332336425781, time passed 164s\n",
      "Epoch 2 - train loss:0.7772764291452325, accuracy:73.35357666015625, time passed 323s\n",
      "Epoch 2 - val loss:0.9126824842223639, accuracy:69.87737274169922, time passed 334s\n",
      "Epoch 3 - train loss:0.6559879050382873, accuracy:77.59510803222656, time passed 491s\n",
      "Epoch 3 - val loss:0.8079986576038071, accuracy:72.99248504638672, time passed 500s\n",
      "Epoch 4 - train loss:0.5719359525481759, accuracy:80.4427719116211, time passed 665s\n",
      "Epoch 4 - val loss:0.7625397760656816, accuracy:74.90110778808594, time passed 678s\n",
      "Epoch 5 - train loss:0.5159824037033579, accuracy:82.39330291748047, time passed 834s\n",
      "Epoch 5 - val loss:0.8050884136670753, accuracy:74.49565124511719, time passed 844s\n",
      "Epoch 6 - train loss:0.454458064816492, accuracy:84.59998321533203, time passed 1006s\n",
      "Epoch 6 - val loss:0.7748858558980725, accuracy:75.1186752319336, time passed 1016s\n",
      "Epoch 7 - train loss:0.40982246905793923, accuracy:86.02581787109375, time passed 1178s\n",
      "Epoch 7 - val loss:0.802957343904278, accuracy:75.52413177490234, time passed 1190s\n",
      "Epoch 8 - train loss:0.374251716315289, accuracy:87.36932373046875, time passed 1352s\n",
      "Epoch 8 - val loss:0.7804073619691632, accuracy:75.69224548339844, time passed 1364s\n",
      "Epoch 9 - train loss:0.33531771962295104, accuracy:88.71803283691406, time passed 1531s\n",
      "Epoch 9 - val loss:0.8005979502502876, accuracy:75.71202850341797, time passed 1543s\n",
      "Epoch 10 - train loss:0.2970279723100955, accuracy:89.94605255126953, time passed 1705s\n",
      "Epoch 10 - val loss:0.8006168351143221, accuracy:76.25593566894531, time passed 1717s\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "writer = SummaryWriter('runs/lab9/resnet')\n",
    "train_evaluate(net, optimizer, writer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 24, 16, 16]             648\n",
      "       BatchNorm2d-2           [-1, 24, 16, 16]              48\n",
      "              ReLU-3           [-1, 24, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 24, 8, 8]               0\n",
      "            Conv2d-5             [-1, 24, 4, 4]             216\n",
      "       BatchNorm2d-6             [-1, 24, 4, 4]              48\n",
      "            Conv2d-7            [-1, 122, 4, 4]           2,928\n",
      "       BatchNorm2d-8            [-1, 122, 4, 4]             244\n",
      "              ReLU-9            [-1, 122, 4, 4]               0\n",
      "           Conv2d-10            [-1, 122, 8, 8]           2,928\n",
      "      BatchNorm2d-11            [-1, 122, 8, 8]             244\n",
      "             ReLU-12            [-1, 122, 8, 8]               0\n",
      "           Conv2d-13            [-1, 122, 4, 4]           1,098\n",
      "      BatchNorm2d-14            [-1, 122, 4, 4]             244\n",
      "           Conv2d-15            [-1, 122, 4, 4]          14,884\n",
      "      BatchNorm2d-16            [-1, 122, 4, 4]             244\n",
      "             ReLU-17            [-1, 122, 4, 4]               0\n",
      " InvertedResidual-18            [-1, 244, 4, 4]               0\n",
      "           Conv2d-19            [-1, 122, 4, 4]          14,884\n",
      "      BatchNorm2d-20            [-1, 122, 4, 4]             244\n",
      "             ReLU-21            [-1, 122, 4, 4]               0\n",
      "           Conv2d-22            [-1, 122, 4, 4]           1,098\n",
      "      BatchNorm2d-23            [-1, 122, 4, 4]             244\n",
      "           Conv2d-24            [-1, 122, 4, 4]          14,884\n",
      "      BatchNorm2d-25            [-1, 122, 4, 4]             244\n",
      "             ReLU-26            [-1, 122, 4, 4]               0\n",
      " InvertedResidual-27            [-1, 244, 4, 4]               0\n",
      "           Conv2d-28            [-1, 122, 4, 4]          14,884\n",
      "      BatchNorm2d-29            [-1, 122, 4, 4]             244\n",
      "             ReLU-30            [-1, 122, 4, 4]               0\n",
      "           Conv2d-31            [-1, 122, 4, 4]           1,098\n",
      "      BatchNorm2d-32            [-1, 122, 4, 4]             244\n",
      "           Conv2d-33            [-1, 122, 4, 4]          14,884\n",
      "      BatchNorm2d-34            [-1, 122, 4, 4]             244\n",
      "             ReLU-35            [-1, 122, 4, 4]               0\n",
      " InvertedResidual-36            [-1, 244, 4, 4]               0\n",
      "           Conv2d-37            [-1, 122, 4, 4]          14,884\n",
      "      BatchNorm2d-38            [-1, 122, 4, 4]             244\n",
      "             ReLU-39            [-1, 122, 4, 4]               0\n",
      "           Conv2d-40            [-1, 122, 4, 4]           1,098\n",
      "      BatchNorm2d-41            [-1, 122, 4, 4]             244\n",
      "           Conv2d-42            [-1, 122, 4, 4]          14,884\n",
      "      BatchNorm2d-43            [-1, 122, 4, 4]             244\n",
      "             ReLU-44            [-1, 122, 4, 4]               0\n",
      " InvertedResidual-45            [-1, 244, 4, 4]               0\n",
      "           Conv2d-46            [-1, 244, 2, 2]           2,196\n",
      "      BatchNorm2d-47            [-1, 244, 2, 2]             488\n",
      "           Conv2d-48            [-1, 244, 2, 2]          59,536\n",
      "      BatchNorm2d-49            [-1, 244, 2, 2]             488\n",
      "             ReLU-50            [-1, 244, 2, 2]               0\n",
      "           Conv2d-51            [-1, 244, 4, 4]          59,536\n",
      "      BatchNorm2d-52            [-1, 244, 4, 4]             488\n",
      "             ReLU-53            [-1, 244, 4, 4]               0\n",
      "           Conv2d-54            [-1, 244, 2, 2]           2,196\n",
      "      BatchNorm2d-55            [-1, 244, 2, 2]             488\n",
      "           Conv2d-56            [-1, 244, 2, 2]          59,536\n",
      "      BatchNorm2d-57            [-1, 244, 2, 2]             488\n",
      "             ReLU-58            [-1, 244, 2, 2]               0\n",
      " InvertedResidual-59            [-1, 488, 2, 2]               0\n",
      "           Conv2d-60            [-1, 244, 2, 2]          59,536\n",
      "      BatchNorm2d-61            [-1, 244, 2, 2]             488\n",
      "             ReLU-62            [-1, 244, 2, 2]               0\n",
      "           Conv2d-63            [-1, 244, 2, 2]           2,196\n",
      "      BatchNorm2d-64            [-1, 244, 2, 2]             488\n",
      "           Conv2d-65            [-1, 244, 2, 2]          59,536\n",
      "      BatchNorm2d-66            [-1, 244, 2, 2]             488\n",
      "             ReLU-67            [-1, 244, 2, 2]               0\n",
      " InvertedResidual-68            [-1, 488, 2, 2]               0\n",
      "           Conv2d-69            [-1, 244, 2, 2]          59,536\n",
      "      BatchNorm2d-70            [-1, 244, 2, 2]             488\n",
      "             ReLU-71            [-1, 244, 2, 2]               0\n",
      "           Conv2d-72            [-1, 244, 2, 2]           2,196\n",
      "      BatchNorm2d-73            [-1, 244, 2, 2]             488\n",
      "           Conv2d-74            [-1, 244, 2, 2]          59,536\n",
      "      BatchNorm2d-75            [-1, 244, 2, 2]             488\n",
      "             ReLU-76            [-1, 244, 2, 2]               0\n",
      " InvertedResidual-77            [-1, 488, 2, 2]               0\n",
      "           Conv2d-78            [-1, 244, 2, 2]          59,536\n",
      "      BatchNorm2d-79            [-1, 244, 2, 2]             488\n",
      "             ReLU-80            [-1, 244, 2, 2]               0\n",
      "           Conv2d-81            [-1, 244, 2, 2]           2,196\n",
      "      BatchNorm2d-82            [-1, 244, 2, 2]             488\n",
      "           Conv2d-83            [-1, 244, 2, 2]          59,536\n",
      "      BatchNorm2d-84            [-1, 244, 2, 2]             488\n",
      "             ReLU-85            [-1, 244, 2, 2]               0\n",
      " InvertedResidual-86            [-1, 488, 2, 2]               0\n",
      "           Conv2d-87            [-1, 244, 2, 2]          59,536\n",
      "      BatchNorm2d-88            [-1, 244, 2, 2]             488\n",
      "             ReLU-89            [-1, 244, 2, 2]               0\n",
      "           Conv2d-90            [-1, 244, 2, 2]           2,196\n",
      "      BatchNorm2d-91            [-1, 244, 2, 2]             488\n",
      "           Conv2d-92            [-1, 244, 2, 2]          59,536\n",
      "      BatchNorm2d-93            [-1, 244, 2, 2]             488\n",
      "             ReLU-94            [-1, 244, 2, 2]               0\n",
      " InvertedResidual-95            [-1, 488, 2, 2]               0\n",
      "           Conv2d-96            [-1, 244, 2, 2]          59,536\n",
      "      BatchNorm2d-97            [-1, 244, 2, 2]             488\n",
      "             ReLU-98            [-1, 244, 2, 2]               0\n",
      "           Conv2d-99            [-1, 244, 2, 2]           2,196\n",
      "     BatchNorm2d-100            [-1, 244, 2, 2]             488\n",
      "          Conv2d-101            [-1, 244, 2, 2]          59,536\n",
      "     BatchNorm2d-102            [-1, 244, 2, 2]             488\n",
      "            ReLU-103            [-1, 244, 2, 2]               0\n",
      "InvertedResidual-104            [-1, 488, 2, 2]               0\n",
      "          Conv2d-105            [-1, 244, 2, 2]          59,536\n",
      "     BatchNorm2d-106            [-1, 244, 2, 2]             488\n",
      "            ReLU-107            [-1, 244, 2, 2]               0\n",
      "          Conv2d-108            [-1, 244, 2, 2]           2,196\n",
      "     BatchNorm2d-109            [-1, 244, 2, 2]             488\n",
      "          Conv2d-110            [-1, 244, 2, 2]          59,536\n",
      "     BatchNorm2d-111            [-1, 244, 2, 2]             488\n",
      "            ReLU-112            [-1, 244, 2, 2]               0\n",
      "InvertedResidual-113            [-1, 488, 2, 2]               0\n",
      "          Conv2d-114            [-1, 244, 2, 2]          59,536\n",
      "     BatchNorm2d-115            [-1, 244, 2, 2]             488\n",
      "            ReLU-116            [-1, 244, 2, 2]               0\n",
      "          Conv2d-117            [-1, 244, 2, 2]           2,196\n",
      "     BatchNorm2d-118            [-1, 244, 2, 2]             488\n",
      "          Conv2d-119            [-1, 244, 2, 2]          59,536\n",
      "     BatchNorm2d-120            [-1, 244, 2, 2]             488\n",
      "            ReLU-121            [-1, 244, 2, 2]               0\n",
      "InvertedResidual-122            [-1, 488, 2, 2]               0\n",
      "          Conv2d-123            [-1, 488, 1, 1]           4,392\n",
      "     BatchNorm2d-124            [-1, 488, 1, 1]             976\n",
      "          Conv2d-125            [-1, 488, 1, 1]         238,144\n",
      "     BatchNorm2d-126            [-1, 488, 1, 1]             976\n",
      "            ReLU-127            [-1, 488, 1, 1]               0\n",
      "          Conv2d-128            [-1, 488, 2, 2]         238,144\n",
      "     BatchNorm2d-129            [-1, 488, 2, 2]             976\n",
      "            ReLU-130            [-1, 488, 2, 2]               0\n",
      "          Conv2d-131            [-1, 488, 1, 1]           4,392\n",
      "     BatchNorm2d-132            [-1, 488, 1, 1]             976\n",
      "          Conv2d-133            [-1, 488, 1, 1]         238,144\n",
      "     BatchNorm2d-134            [-1, 488, 1, 1]             976\n",
      "            ReLU-135            [-1, 488, 1, 1]               0\n",
      "InvertedResidual-136            [-1, 976, 1, 1]               0\n",
      "          Conv2d-137            [-1, 488, 1, 1]         238,144\n",
      "     BatchNorm2d-138            [-1, 488, 1, 1]             976\n",
      "            ReLU-139            [-1, 488, 1, 1]               0\n",
      "          Conv2d-140            [-1, 488, 1, 1]           4,392\n",
      "     BatchNorm2d-141            [-1, 488, 1, 1]             976\n",
      "          Conv2d-142            [-1, 488, 1, 1]         238,144\n",
      "     BatchNorm2d-143            [-1, 488, 1, 1]             976\n",
      "            ReLU-144            [-1, 488, 1, 1]               0\n",
      "InvertedResidual-145            [-1, 976, 1, 1]               0\n",
      "          Conv2d-146            [-1, 488, 1, 1]         238,144\n",
      "     BatchNorm2d-147            [-1, 488, 1, 1]             976\n",
      "            ReLU-148            [-1, 488, 1, 1]               0\n",
      "          Conv2d-149            [-1, 488, 1, 1]           4,392\n",
      "     BatchNorm2d-150            [-1, 488, 1, 1]             976\n",
      "          Conv2d-151            [-1, 488, 1, 1]         238,144\n",
      "     BatchNorm2d-152            [-1, 488, 1, 1]             976\n",
      "            ReLU-153            [-1, 488, 1, 1]               0\n",
      "InvertedResidual-154            [-1, 976, 1, 1]               0\n",
      "          Conv2d-155            [-1, 488, 1, 1]         238,144\n",
      "     BatchNorm2d-156            [-1, 488, 1, 1]             976\n",
      "            ReLU-157            [-1, 488, 1, 1]               0\n",
      "          Conv2d-158            [-1, 488, 1, 1]           4,392\n",
      "     BatchNorm2d-159            [-1, 488, 1, 1]             976\n",
      "          Conv2d-160            [-1, 488, 1, 1]         238,144\n",
      "     BatchNorm2d-161            [-1, 488, 1, 1]             976\n",
      "            ReLU-162            [-1, 488, 1, 1]               0\n",
      "InvertedResidual-163            [-1, 976, 1, 1]               0\n",
      "          Conv2d-164           [-1, 2048, 1, 1]       1,998,848\n",
      "     BatchNorm2d-165           [-1, 2048, 1, 1]           4,096\n",
      "            ReLU-166           [-1, 2048, 1, 1]               0\n",
      "          Linear-167                   [-1, 10]          20,490\n",
      "    ShuffleNetV2-168                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 5,365,486\n",
      "Trainable params: 5,365,486\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.88\n",
      "Params size (MB): 20.47\n",
      "Estimated Total Size (MB): 22.36\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = models.shufflenet_v2_x2_0()\n",
    "        self.net.trainable = False # Freeze the layers of the pretrained model\n",
    "        self.net.fc = nn.Linear(2048, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "shuff = Net().to(device)\n",
    "summary(shuff, (3, 32, 32))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - train loss:1.8099873870839853, accuracy:34.54203796386719, time passed 69s\n",
      "Epoch 1 - val loss:1.5894068539897097, accuracy:42.592960357666016, time passed 77s\n",
      "Epoch 2 - train loss:1.5120637828431776, accuracy:45.88515090942383, time passed 142s\n",
      "Epoch 2 - val loss:1.5226520858233488, accuracy:43.769779205322266, time passed 150s\n",
      "Epoch 3 - train loss:1.3453806385664684, accuracy:51.73833084106445, time passed 217s\n",
      "Epoch 3 - val loss:1.3490767282775686, accuracy:51.918514251708984, time passed 227s\n",
      "Epoch 4 - train loss:1.2362519484346786, accuracy:55.862770080566406, time passed 298s\n",
      "Epoch 4 - val loss:1.3394668162623538, accuracy:52.07674026489258, time passed 305s\n",
      "Epoch 5 - train loss:1.1570540808350838, accuracy:58.93901824951172, time passed 373s\n",
      "Epoch 5 - val loss:1.2708812552162363, accuracy:55.696205139160156, time passed 381s\n",
      "Epoch 6 - train loss:1.0803683448935408, accuracy:61.6032600402832, time passed 449s\n",
      "Epoch 6 - val loss:1.2300076899649222, accuracy:57.001583099365234, time passed 457s\n",
      "Epoch 7 - train loss:1.0260539018284633, accuracy:63.69804763793945, time passed 531s\n",
      "Epoch 7 - val loss:1.2359657061250904, accuracy:56.79391098022461, time passed 539s\n",
      "Epoch 8 - train loss:0.9850362746611886, accuracy:65.39442443847656, time passed 607s\n",
      "Epoch 8 - val loss:1.1509046139596384, accuracy:59.909019470214844, time passed 616s\n",
      "Epoch 9 - train loss:0.9007433698610272, accuracy:68.15576934814453, time passed 689s\n",
      "Epoch 9 - val loss:1.119989833499812, accuracy:60.81883239746094, time passed 699s\n",
      "Epoch 10 - train loss:0.852950286834746, accuracy:70.29691314697266, time passed 776s\n",
      "Epoch 10 - val loss:1.1306565969805173, accuracy:61.698974609375, time passed 786s\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(shuff.parameters())\n",
    "writer = SummaryWriter('runs/lab9/shuff')\n",
    "train_evaluate(shuff, optimizer, writer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "Launching TensorBoard..."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir runs/lab9"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "7d2acab730e340328275a65d7aad46cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_648f2e5785c94231a90fcdae2b6c4259",
       "IPY_MODEL_61da3848bcbc4ac4b390a0dd933cfc2e",
       "IPY_MODEL_b3fc7d5e4ba743878e72117ccbb59921"
      ],
      "layout": "IPY_MODEL_dba3174329364f9c997be3ed52805898"
     }
    },
    "648f2e5785c94231a90fcdae2b6c4259": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_234a4d772c654ade8fa221d5a82fe986",
      "placeholder": "​",
      "style": "IPY_MODEL_e2873a5ff41a44038f163e8345203ebf",
      "value": "100%"
     }
    },
    "61da3848bcbc4ac4b390a0dd933cfc2e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_51310412110a41d89b328e96228c0fe5",
      "max": 170498071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d609c3e38828488cbe102d54b652f25b",
      "value": 170498071
     }
    },
    "b3fc7d5e4ba743878e72117ccbb59921": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_46ac91c05ac04b4286239ea3374463f8",
      "placeholder": "​",
      "style": "IPY_MODEL_085cbcb85d084380a11e730436658be3",
      "value": " 170498071/170498071 [00:04&lt;00:00, 47623533.78it/s]"
     }
    },
    "dba3174329364f9c997be3ed52805898": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "234a4d772c654ade8fa221d5a82fe986": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e2873a5ff41a44038f163e8345203ebf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "51310412110a41d89b328e96228c0fe5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d609c3e38828488cbe102d54b652f25b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "46ac91c05ac04b4286239ea3374463f8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "085cbcb85d084380a11e730436658be3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9f92ed155f904c75a3769753b7dc2af3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_09630dae76034a4a92a58bc35c21de40",
       "IPY_MODEL_549f9fd7e8054f53bba76835de79f058",
       "IPY_MODEL_2e23b10ab2e548388c3ff12b82183980"
      ],
      "layout": "IPY_MODEL_a04747107b7548b3a40229b7f6c57666"
     }
    },
    "09630dae76034a4a92a58bc35c21de40": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_03cba228644547e6b4a737fc1cbf9ec1",
      "placeholder": "​",
      "style": "IPY_MODEL_9ae111eaa11f42f8a3fce2954f175adb",
      "value": "100%"
     }
    },
    "549f9fd7e8054f53bba76835de79f058": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ea935600ebca40a2be8b33734ddff5f8",
      "max": 170498071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_88c9652c16d547b09e33c515d790c952",
      "value": 170498071
     }
    },
    "2e23b10ab2e548388c3ff12b82183980": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_48e37ef977304ea9a41e325819e34be4",
      "placeholder": "​",
      "style": "IPY_MODEL_4f3fdbcbc3fa4542abf12925a0611ece",
      "value": " 170498071/170498071 [00:02&lt;00:00, 74016671.51it/s]"
     }
    },
    "a04747107b7548b3a40229b7f6c57666": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "03cba228644547e6b4a737fc1cbf9ec1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9ae111eaa11f42f8a3fce2954f175adb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ea935600ebca40a2be8b33734ddff5f8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "88c9652c16d547b09e33c515d790c952": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "48e37ef977304ea9a41e325819e34be4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4f3fdbcbc3fa4542abf12925a0611ece": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  },
  "gpuClass": "standard",
  "accelerator": "TPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
