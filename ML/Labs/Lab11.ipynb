{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PZKrlwajuGZ-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Week 11: Ensemble Learning\n",
    "\n",
    "```\n",
    "- Machine Learning, Innopolis University (Fall semester 2022)\n",
    "- Professor: Adil Khan\n",
    "- Teaching Assistant: Gcinizwe Dlamini\n",
    "```\n",
    "<hr>\n",
    "\n",
    "\n",
    "```\n",
    "In this lab, you will practice Ensemble learning\n",
    "\n",
    "Lab Plan :\n",
    "    1. Ensemble learning\n",
    "    2. Bagging\n",
    "    3. Random Forests\n",
    "    4. AdaBoost\n",
    "```\n",
    "\n",
    "<hr>\n",
    "\n",
    "Why ensemble learning? How does it help? <span style=\"color:blue\"> By combining the power of multiple models in a single model while overcoming their weaknesses, thus reducing variance and/or bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4G2NXb2yznzd",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Ensemble learning\n",
    "We will explore ensemble learning on the example of decision trees - we will see how ensembles can improve classification accuracy.\n",
    "\n",
    "Let's start from uploading MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 213
    },
    "colab_type": "code",
    "id": "9sI82NDtzoSP",
    "outputId": "a3162ec7-b6cb-4258-dcc6-86eca1157e99",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)\n",
    "\n",
    "\n",
    "plt.figure(1, figsize=(3, 3))\n",
    "plt.imshow(X[1].reshape((8,8)), cmap=\"gray\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title(f\"label is {y[1]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TcJSouftKwUC",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Single decision tree\n",
    "\n",
    "First, we train a single decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "aqrQbHVDKw9F",
    "outputId": "c7ebcbc8-41cd-49c3-8d44-5bed82c74265",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single tree accuracy: 0.8619528619528619\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X_train, y_train)\n",
    "pred = tree.predict(X_test)\n",
    "tree_score = accuracy_score(y_test, pred)\n",
    "print(\"Single tree accuracy:\", tree_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qgAj4l5GLKuG",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note the accuracy - it is around **0.85**.\n",
    "\n",
    "## 1. Bagging\n",
    "\n",
    "\n",
    "What is decreased by bagging? Variance or bias? How? <span style=\"color:blue\"> Averaging reduces variance.\n",
    "$$Var\\left(\\frac{1}{n}\\sum_{n=1}^{n}x_i\\right) = \\frac{1}{n^2}var\\left(\\sum_{n=1}^{n}x_i\\right) = \\frac{1}{n^2}\\left(\\sum_{n=1}^{n}var\\left(x_i\\right)\\right) = \\frac{1}{n^2} \\sum_{n=1}^{n} \\sigma^2 = \\frac{1}{n^2} * n * \\sigma^2 = \\frac{\\sigma^2}{n}$$\n",
    "    \n",
    "\n",
    "\n",
    "Now let's improve it a bit by the means of bagging. We train a hundred of independent classifiers and make a prediction by majority voting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8 2 2 ... 2 2 2]\n",
      " [8 8 8 ... 8 8 8]\n",
      " [2 2 2 ... 2 2 2]\n",
      " ...\n",
      " [1 1 1 ... 0 9 9]\n",
      " [1 8 1 ... 1 1 1]\n",
      " [2 2 2 ... 2 2 2]]\n",
      "Bagging accuracy: 0.9511784511784511\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "\n",
    "n_trees = 100\n",
    "\n",
    "classifiers = []\n",
    "for i in range(n_trees):\n",
    "    # train a new classifier and append it to the list\n",
    "    indx = np.random.randint(0,len(y_train),len(y_train))\n",
    "    tree = DecisionTreeClassifier()\n",
    "    tree.fit(X_train[indx], y_train[indx])\n",
    "    classifiers.append(tree)\n",
    "\n",
    "# here we will store predictions for all samples and all base classifiers\n",
    "base_pred = np.zeros((X_test.shape[0], n_trees), dtype=\"int\")\n",
    "for i in range(n_trees):\n",
    "    # obtain the predictions from each tree\n",
    "    base_pred[:,i] = classifiers[i].predict(X_test)\n",
    "\n",
    "print(base_pred)\n",
    "\n",
    "# aggregate predictions by majority voting\n",
    "pred = mode(base_pred, axis=1)[0].ravel()\n",
    "acc = accuracy_score(y_test, pred)\n",
    "print(\"Bagging accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "colab_type": "code",
    "id": "xWQTCYrGLLMM",
    "outputId": "c762f1dd-631c-40d3-f1cd-d45855890388",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 8 8 ... 3 8 3]\n",
      " [8 8 8 ... 8 8 8]\n",
      " [2 2 2 ... 2 2 2]\n",
      " ...\n",
      " [1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 1]\n",
      " [2 2 2 ... 2 2 2]]\n",
      "Bagging accuracy: 0.8821548821548821\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "\n",
    "n_trees = 100\n",
    "\n",
    "classifiers = []\n",
    "for i in range(n_trees):\n",
    "    # train a new classifier and append it to the list\n",
    "    tree = DecisionTreeClassifier()\n",
    "    tree.fit(X_train, y_train)\n",
    "    classifiers.append(tree)\n",
    "\n",
    "# here we will store predictions for all samples and all base classifiers\n",
    "base_pred = np.zeros((X_test.shape[0], n_trees), dtype=\"int\")\n",
    "for i in range(n_trees):\n",
    "    # obtain the predictions from each tree\n",
    "    base_pred[:,i] = classifiers[i].predict(X_test)\n",
    "\n",
    "print(base_pred)\n",
    "\n",
    "# aggregate predictions by majority voting\n",
    "pred = mode(base_pred, axis=1)[0].ravel()\n",
    "acc = accuracy_score(y_test, pred)\n",
    "print(\"Bagging accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ODOPvJJ2bKPh",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now the accuracy grew up to **0.88**. You can see that our classifiers return very similar results. By the way, why the base classifiers are not identical at all? <span style=\"color:blue\"> That is the case, if the improvement of the criterion is identical for several splits and one split has to be selected at random.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Random forest\n",
    "\n",
    "Compared to simple bagging we've just implemented, random forest can show better results because base classifiers are much less correlated.\n",
    "\n",
    "<span style=\"color:red\">TODO: At first, implement bootstrap sampling (use numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "C9d1ElFpE48c",
    "outputId": "9c6fc189-e7fa-4b2a-a5db-41403aab23c4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0,  1,  2],\n",
       "        [ 9, 10, 11],\n",
       "        [ 3,  4,  5],\n",
       "        [ 0,  1,  2]]),\n",
       " array([0, 3, 1, 0]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bootstrap(X, y):\n",
    "    # TODO: generate bootstrap indices and return data according to them\n",
    "    return None\n",
    "\n",
    "\n",
    "# this is a test, will work if you are using np.random.randint() for indices generation\n",
    "np.random.seed(0)\n",
    "a = np.array(range(12)).reshape(4,3)\n",
    "b = np.array(range(4))\n",
    "bootstrap(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NxpCck94Y73A",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You should get\n",
    "\n",
    "(array([[ 0,  1,  2], <br>\n",
    "&emsp;&emsp;&emsp;[ 9, 10, 11], <br>\n",
    "&emsp;&emsp;&emsp;[ 3,  4,  5], <br>\n",
    "&emsp;&emsp;&emsp;[ 0,  1,  2]]), <br>\n",
    "array([0, 3, 1, 0]))\n",
    "       \n",
    "Now let's build a set of decision trees, each of them is trained on a bootstrap sampling from X and $\\sqrt d$ features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HxIhmI_H5jnI",
    "outputId": "b2e3e694-f1a1-4513-b281-f7acac8febe1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest accuracy: 0.9814814814814815\n"
     ]
    }
   ],
   "source": [
    "classifiers = []\n",
    "for i in range(n_trees):\n",
    "    # train a new tree on sqrt(n_features) and bootstrapped data, append it to the list\n",
    "    base = DecisionTreeClassifier(max_features=\"sqrt\")\n",
    "    bs_X, bs_y = bootstrap(X_train, y_train)\n",
    "    base.fit(bs_X, bs_y)\n",
    "    classifiers.append(base)\n",
    "\n",
    "base_pred = np.zeros((n_trees, X_test.shape[0]), dtype=\"int\")\n",
    "for i in range(n_trees):\n",
    "    base_pred[i,:] = classifiers[i].predict(X_test)\n",
    "\n",
    "pred = mode(base_pred, axis=0)[0].ravel()\n",
    "acc = accuracy_score(y_test, pred)\n",
    "print(\"Random forest accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ObGBbQfrE5jM",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "And now we got **0.97** accuracy, which is a significant improvement! Now you can see why it is so important to have diverse classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qp4WWdYezpHy",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. Boosting\n",
    "\n",
    "How does boosting work? <span style=\"color:blue\"> Models are built sequentially: each model is built using information from previously built models. Boosting does not involve bootstrap sampling; instead each tree is fit on a modified version of the original data set.\n",
    "\n",
    "For simplicity let's make a binary classification problem out of the original problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JBbO7p-aNk69",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_train_b = (y_train == 2 ) * 2 - 1\n",
    "y_test_b = (y_test == 2 ) * 2 - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "talvfivTQitE",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now let's train a boosting model.\n",
    "\n",
    "We will have sample weights and tree weights. Initially all sample weights are equal. After that we will increase weight for complicated samples.\n",
    "\n",
    "Tree weight $w$ is computed using weighted error or $1 - accuracy$\n",
    "\n",
    "$w_t = \\frac12 log(\\frac{1-weighted\\_error_t}{weighted\\_error_t})$ for each base classifier.\n",
    "\n",
    "For correct samples weights will be decreased $e^w$ times, and for incorrect classified samples increased  $e^w$ times. After this changes we normalize weights.\n",
    "\n",
    "\n",
    "## 3.1 Boosting from Scratch \n",
    "\n",
    "Tasks: \n",
    "1. initialize sample weights\n",
    "1. caclulate tree weight\n",
    "1. update sample weights\n",
    "1. normalize the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mVEBRi4pEwte",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_trees = 10\n",
    "tree_weights = np.zeros(n_trees)\n",
    "classifiers = []\n",
    "train_samples = X_train.shape[0]\n",
    "\n",
    "# TODO: initialize sample weights\n",
    "sample_weights = None\n",
    "\n",
    "\n",
    "for i in range(n_trees):\n",
    "    clf = DecisionTreeClassifier(min_samples_leaf=3)\n",
    "    clf.fit(X_train, y_train_b, sample_weight=sample_weights)\n",
    "    pred = clf.predict(X_train)\n",
    "    acc = accuracy_score(y_train_b, pred, sample_weight=sample_weights)\n",
    "    \n",
    "    # TODO: caclulate tree weight\n",
    "    w = None\n",
    "    tree_weights[i] = w\n",
    "    classifiers.append(clf)\n",
    "    \n",
    "    # TODO: update sample weights\n",
    "    for j in range(train_samples):\n",
    "        if pred[j] != y[j]:\n",
    "            sample_weights[j] = None\n",
    "        else:\n",
    "            sample_weights[j] = None\n",
    "    \n",
    "    # TODO: normalize the weights\n",
    "    sample_weights = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YJ43LIorSXbs",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Use trees voting to calculate final predictions. Since we have a binary classification, the prediction will be calculated as follows:\n",
    "\n",
    "$\\hat{y} = sign(\\sum_{t=1}^{T}(w_t f_t(x)))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PpJKjCDdzpmt",
    "outputId": "4d7db356-1a10-413f-c432-1877b3f46b72",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosting accuracy: 0.9730639730639731\n"
     ]
    }
   ],
   "source": [
    "n_test = X_test.shape[0]\n",
    "\n",
    "pred = np.zeros(n_test)\n",
    "# caclulate predictions\n",
    "for t in range(n_trees):\n",
    "    pred += classifiers[t].predict(X_test) * tree_weights[t]\n",
    "for i in range(n_test):\n",
    "    pred[i] = 1 if pred[i] > 0 else -1\n",
    "\n",
    "\n",
    "acc = accuracy_score(y_test_b, pred)\n",
    "print(\"Boosting accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E7mv1TfwSahW",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The resulting accuracy is **0.97**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Self practice task\n",
    "\n",
    "Using dataset from assignment 2 (Task 1)\n",
    "* Train and evaluate the following models using default parameters:\n",
    "    1. Decision Tree\n",
    "    1. Random Forest\n",
    "    1. Adaptive boosting model\n",
    "    1. [Catboost from Yandex](https://catboost.ai/en/docs/concepts/python-quickstart)\n",
    "    1. [LightGBM](https://lightgbm.readthedocs.io/en/v3.3.2/)\n",
    "* Apply hyperparameters tuning for the models listed above and compare the models performance and also training time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab7_answers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}